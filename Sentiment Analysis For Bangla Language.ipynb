{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classDict = {'neg': 0, 'pos': 1,'ntr': 2}\n",
    "label2id = []\n",
    "def read_File():\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    scrip_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "    rel_path = \"data.txt\"\n",
    "    abs_file_path = os.path.join(scrip_dir, rel_path)\n",
    "    with open(abs_file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word = line.split()\n",
    "            labels.append(word[0])\n",
    "            label2id.append(classDict[word[0]])\n",
    "            sentence = re.sub(word[0]+'\\t', '', line)\n",
    "            sentences.append(sentence)\n",
    "    print('read file complete')\n",
    "    \n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file complete\n"
     ]
    }
   ],
   "source": [
    "sentences, labels = read_File()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos</td>\n",
       "      <td>একজন বাংলাদেশী হিসাবে আমি গর্বিত । আরও গর্বিত ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>কি বিচিত্র এই দেশ,   আর একজন ক্বারি ওবায়দুল্ল...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>আলহামদুলিল্লাহ একজন বাঙ্গালী হিসাবে গর্ব বোধ ক...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>অভিনন্দন তোমা‌কে মুস্তা‌ফিজ, তু‌মি এর উপযুক্ত...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos</td>\n",
       "      <td>আলহামদুলিল্লাহ। অভিনন্দন সাতক্ষীরা বাসির অহংকা...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                           sentence\n",
       "0   pos  একজন বাংলাদেশী হিসাবে আমি গর্বিত । আরও গর্বিত ...\n",
       "1   neg  কি বিচিত্র এই দেশ,   আর একজন ক্বারি ওবায়দুল্ল...\n",
       "2   pos  আলহামদুলিল্লাহ একজন বাঙ্গালী হিসাবে গর্ব বোধ ক...\n",
       "3   pos  অভিনন্দন তোমা‌কে মুস্তা‌ফিজ, তু‌মি এর উপযুক্ত...\n",
       "4   pos  আলহামদুলিল্লাহ। অভিনন্দন সাতক্ষীরা বাসির অহংকা..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'label': labels,\n",
    "        'sentence': sentences\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['label', 'sentence'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2449</td>\n",
       "      <td>2449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3</td>\n",
       "      <td>2419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>neg</td>\n",
       "      <td>আলহামদুলিল্লাহ একজন বাঙ্গালী হিসাবে গর্ব বোধ ক...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1367</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                           sentence\n",
       "count   2449                                               2449\n",
       "unique     3                                               2419\n",
       "top      neg  আলহামদুলিল্লাহ একজন বাঙ্গালী হিসাবে গর্ব বোধ ক...\n",
       "freq    1367                                                  3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f31d31a77b8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['label'].value_counts().plot(kind=\"bar\", rot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sent):\n",
    "    \n",
    "    sent = re.sub('[?.`*^()°¢܌Ͱ̰ߒנ~×Ҡߘ:ҰߑÍ|।;!,&%\\'@#$><A-Za-z0+-9=./''\"\"_০-৯]', '', sent)\n",
    "    sent = re.sub(r'(\\W)(?=\\1)', '', sent)\n",
    "    sent = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', sent, flags=re.MULTILINE)\n",
    "    sent = re.sub(r'\\<a href', ' ', sent)\n",
    "    sent = re.sub(r'&amp;', '', sent) \n",
    "    sent = re.sub(r'<br />', ' ', sent)\n",
    "    sent = re.sub(r'\\'', ' ', sent)\n",
    "    sent = re.sub(r'ߑͰߑ̰ߒנ', '', sent)\n",
    "    sent = re.sub(r'ߎɰߎɰߎɍ', '', sent)\n",
    "    sent = sent.strip()\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'আর একটা কথা না বললেই নয়,আই ছি ছি সব সময় বাংলাদেশের বিপক্ষে অবস্থান নেয়।এভাবে চলতে থাকলে বাংলাদেশ টিম ধ্বংস হবে।\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[15,'sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sent  in enumerate(df['sentence'].tolist()):\n",
    "    df.loc[i,'sentence'] = clean_sentence(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'আর একটা কথা না বললেই নয়আই ছি ছি সব সময় বাংলাদেশের বিপক্ষে অবস্থান নেয়এভাবে চলতে থাকলে বাংলাদেশ টিম ধ্বংস হবে'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df.loc[15,'sentence']\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenized Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_data(sent):\n",
    "    tokenized_text = sent.split()\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {'এ', 'হয়', 'কি', 'কী', 'এর', 'কে', 'যে', 'এই', 'বা', 'সব', 'টি', 'তা',\n",
    "       'সে', 'তাই', 'সেই', 'তার', 'আগে', 'যদি', 'আছে', 'আমি', 'এবং', 'করে', 'কার', 'এটি', 'হতে', 'যায়',\n",
    "       'আরও', 'যাক', 'খুব', 'উপর', 'পরে', 'হবে', 'কেন', 'কখন', 'সকল', 'হয়', 'ঠিক', 'একই', 'কোন',\n",
    "       'ছিল', 'খুবই', 'কোনো', 'অধীন', 'যারা', 'তারা', 'গুলি', 'তাকে', 'সেটা', 'সময়', 'আমার', 'আমরা', 'সবার',\n",
    "       'উভয়', 'একটা', 'আপনি', 'নিয়ে', 'একটি', 'বন্ধ', 'জন্য', 'শুধু', 'যেটা', 'উচিত', 'মাঝে', 'থেকে', 'করবে',\n",
    "       'আবার', 'উপরে', 'সেটি', 'কিছু', 'কারণ', 'যেমন', 'তিনি', 'মধ্যে', 'আমাকে', 'করছেন', 'তুলনা', 'তারপর',\n",
    "       'নিজেই', 'থাকার', 'নিজের', 'পারেন', 'একবার', 'সঙ্গে', 'ইচ্ছা', 'নীচের', 'এগুলো', 'আপনার', 'অধীনে', 'কিংবা',\n",
    "       'এখানে', 'তাহলে', 'কয়েক', 'জন্যে', 'হচ্ছে', 'তাদের', 'কোথায়', 'কিন্তু', 'নিজেকে', 'যতক্ষণ', 'আমাদের',\n",
    "       'দ্বারা', 'হয়েছে', ' সঙ্গে', 'সেখানে', 'কিভাবে', 'মাধ্যমে', 'নিজেদের', 'তুলনায়', 'প্রতিটি',\n",
    "       'তাদেরকে', 'ইত্যাদি', 'সম্পর্কে', 'সর্বাধিক', 'বিরুদ্ধে', 'অন্যান্য'}\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "befor : আর একটা কথা না বললেই নয়আই ছি ছি সব সময় বাংলাদেশের বিপক্ষে অবস্থান নেয়এভাবে চলতে থাকলে বাংলাদেশ টিম ধ্বংস হবে\n",
      "after : আর কথা না বললেই নয়আই ছি ছি সময় বাংলাদেশের বিপক্ষে অবস্থান নেয়এভাবে চলতে থাকলে বাংলাদেশ টিম ধ্বংস\n"
     ]
    }
   ],
   "source": [
    "t_data = tokenized_data(test)\n",
    "r_word = remove_stop_words(t_data)\n",
    "print('befor :', test)\n",
    "print('after :', r_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word to Vector transformation by ngram approch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41476\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "stpGram = {}\n",
    "word_vectorizerGram = CountVectorizer(analyzer='word', ngram_range=(1, 2), min_df=1, lowercase=False,\n",
    "                                          token_pattern=u'[\\S]+', tokenizer=None)\n",
    "\n",
    "\n",
    "df['clean_data'] = [tokenized_data(sent) for sent in df['sentence'].tolist()]\n",
    "\n",
    "word_vectorizerGram.fit_transform(df['sentence'])\n",
    "stpGram = word_vectorizerGram.get_feature_names()\n",
    "\n",
    "\n",
    "\n",
    "print(len(stpGram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36688\n",
      "sample of bigram :  অংশটুকু ফাইন\n"
     ]
    }
   ],
   "source": [
    "df['clean_data'] = [remove_stop_words(tokenized_data(sent)) for sent in df['sentence'].tolist()]\n",
    "\n",
    "word_vectorizerGram_rsw = CountVectorizer(analyzer='word', ngram_range=(1, 2), min_df=1, lowercase=False,\n",
    "                                          token_pattern=u'[\\S]+', tokenizer=None)\n",
    "\n",
    "word_vectorizerGram_rsw.fit_transform(df['clean_data'])\n",
    "stpGram_rsw = word_vectorizerGram_rsw.get_feature_names()\n",
    "\n",
    "print(len(stpGram_rsw))\n",
    "print('sample of bigram : ',stpGram_rsw[20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sentence_to_vector_transform(line):\n",
    "    vec = CountVectorizer(analyzer='word', ngram_range=(1, 2), min_df=1, lowercase=False, token_pattern=u'[\\S]+',\n",
    "                            tokenizer=None, vocabulary=stpGram_rsw)\n",
    "    tList = []\n",
    "    tList.append(line)\n",
    "    sent = vec.transform(tList)\n",
    "    sent = sent.toarray()\n",
    "    sent = np.squeeze(np.asarray(sent))\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataForSVM = []\n",
    "colName = []\n",
    "def sentence_to_vector(data):\n",
    "    dataVec = []\n",
    "    dataVec.append(colName)\n",
    "    \n",
    "    for idx, sent in enumerate(data):\n",
    "        \n",
    "        sent = sentence_to_vector_transform(sent)\n",
    "        sentLst = list(sent)\n",
    "        dtList = []\n",
    "        try:\n",
    "            dtList.append(label2id[idx])\n",
    "            dataForSVM.append(sent)\n",
    "        except:\n",
    "            print(idx)\n",
    "        for item in sentLst:\n",
    "            dtList.append(item)\n",
    "        lengthOfEV = len(dtList)\n",
    "        dataVec.append(dtList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([0, 0, 0, ..., 0, 0, 0])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_vector(df['clean_data'])\n",
    "dataForSVM[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
